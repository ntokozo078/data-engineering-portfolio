{
  "personal": {
    "name": "Ntokozo Ntombela",
    "title": "Data Engineer | ICT Graduate",
    "location": "Durban, South Africa",
    "summary": "A dedicated ICT Graduate from DUT with a strong portfolio in data engineering, automated ETL pipelines, and cloud-native analytics. Transitioning from software engineering to data, I focus on building efficient systems that turn raw data into actionable insights using Python, SQL, Azure, and Databricks."
  },
  "contact": {
    "email": "ntombelan098@gmail.com",
    "github": "https://github.com/ntokozo078",
    "linkedin": "https://www.linkedin.com/in/ntokozo-ntombela-ba662235a/",
    "portfolio": "https://ntokozo078.github.io"
  },
  "specialities": {
    "core": [
      "Data Engineering Pipelines",
      "Dimensional Modeling (Star Schema)",
      "ETL/ELT Automation",
      "Medallion Architecture",
      "Apache Spark (PySpark)",
      "Databricks Lakehouse",
      "Azure Data Services",
      "Advanced SQL & PostgreSQL",
      "REST API Integration",
      "Flask (Backend)",
      "AI Model Integration",
      "Containerization (Docker)"
    ],
    "programming": [
      "Python",
      "SQL",
      "C#",
      "Java",
      "HTML/CSS",
      "JavaScript",
      "Bash/Shell"
    ],
    "cloud_platforms": [
      "Azure",
      "Databricks",
      "Render (Cloud Hosting)",
      "Firebase",
      "AWS (Foundational)"
    ],
    "tools": [
      "Git & GitHub",
      "VS Code",
      "Streamlit",
      "Power BI",
      "Jupyter Notebooks",
      "Docker",
      "PostgreSQL",
      "Cron Jobs"
    ]
  },
  "certifications": {
    "completed": [
      {
        "name": "IBM Spark Fundamentals I",
        "issuer": "IBM SkillsBuild",
        "date": "Nov 2025",
        "status": "completed",
        "icon": "✔",
        "focus": "Big Data"
      },
      { 
        "name": "Academy Accreditation - Databricks Fundamentals", 
        "issuer": "Databricks", 
        "date": "Nov 2025", 
        "focus": "Data Intelligence Platform" 
      },
      {
        "name": "Microsoft Azure Data Fundamentals (DP-900)",
        "issuer": "Microsoft",
        "date": "2025",
        "status": "completed",
        "icon": "✔",
        "focus": "Cloud Data"
      },
      {
        "name": "Databricks for Data Engineering",
        "issuer": "Databricks",
        "date": "Nov 2025",
        "status": "completed",
        "icon": "✔",
        "focus": "Lakehouse Architecture"
      },
      {
        "name": "FNB App Academy 2025",
        "issuer": "FNB",
        "date": "2025",
        "status": "completed",
        "icon": "✔",
        "focus": "Full Stack Engineering"
      },
      {
        "name": "Introduction to Cybersecurity",
        "issuer": "Cisco Networking Academy",
        "date": "May 2025",
        "status": "completed",
        "icon": "✔",
        "focus": "Security"
      },
      {
        "name": "Linux Unhatched",
        "issuer": "Cisco Networking Academy",
        "date": "Oct 2024",
        "status": "completed",
        "icon": "✔",
        "focus": "Infrastructure"
      }
    ],
    "in_progress": [
      {
        "name": "Azure Data Engineer Associate (DP-203)",
        "status": "in_progress",
        "icon": "⚠"
      }
    ],
    "planned": [
      {
        "name": "Databricks Data Engineer Associate",
        "status": "planned",
        "icon": "⭐"
      }
    ]
  },
  "projects": [
    {
      "name": "InsightOps – E-commerce BI System",
      "subtitle": "Data Warehousing & BI",
      "tech_stack": [
        "Python",
        "SQLite",
        "Star Schema",
        "Streamlit",
        "Pandas",
        "Plotly"
      ],
      "summary": "A production-grade Business Intelligence system implementing a Star Schema data warehouse with surrogate keys. It features a full ETL pipeline that transforms raw e-commerce data into actionable KPIs (Revenue, Churn, Profit) and visualizes them on an interactive dashboard with automated alerts.",
      "live_url": "https://ntokozo078--insightops-e-commerce-pipelines-dashboardapp-vbz2ih.streamlit.app/",
      "github_url": "https://github.com/ntokozo078/InsightOps_E-commerce-pipelines",
      "outcome": "Built a scalable dimensional model and automated reporting system, solving real business problems like churn analysis and profit tracking.",
      "type": "Data Warehousing",
      "priority": 1
    },
    {
      "name": "Databricks Medallion Sales Pipeline",
      "subtitle": "Cloud Big Data",
      "tech_stack": [
        "Databricks",
        "PySpark",
        "Delta Lake",
        "Medallion Architecture"
      ],
      "summary": "An end-to-end cloud pipeline processing sales data. Implemented a Bronze (Raw) → Silver (Cleaned) → Gold (Aggregated) workflow using PySpark and Delta Tables to ensure data quality for business reporting.",
      "github_url": "https://github.com/ntokozo078/databricks-medallion-sales-pipeline",
      "outcome": "Demonstrated mastery of modern Lakehouse architecture principles.",
      "type": "Cloud Engineering",
      "priority": 2
    },
    {
      "name": "SA Job Market Pipeline & Tracker",
      "subtitle": "Advanced ETL & Analytics",
      "tech_stack": [
        "Python",
        "PostgreSQL",
        "Flask",
        "BeautifulSoup",
        "Render Cron"
      ],
      "summary": "A robust daily automated pipeline that aggregates tech jobs from APIs and web scrapers. Implemented a 'Zombie Job' filter to remove expired listings, deduplication logic, and SCD Type 1 storage in PostgreSQL.",
      "live_url": "https://devpulse-job-tracker.onrender.com",
      "github_url": "https://github.com/ntokozo078/devpulse-job-tracker",
      "outcome": "Automated the entire data lifecycle from ingestion to visualization, creating a clean historical dataset of the SA tech market.",
      "type": "Data Engineering",
      "priority": 3
    },
    {
      "name": "AI Lyric Video Generator",
      "subtitle": "AI Pipeline & Containerization",
      "tech_stack": [
        "Flask",
        "Faster-Whisper",
        "MoviePy",
        "Docker",
        "FFmpeg"
      ],
      "summary": "A cloud-native media processing app. Integrates the Faster-Whisper AI model for precise audio transcription and MoviePy for dynamic text overlays. Containerized with Docker to run efficiently on low-memory cloud tiers.",
      "live_url": "https://lyric-video-gen.onrender.com",
      "github_url": "https://github.com/ntokozo078/lyric-video-gen",
      "outcome": "Successfully deployed a resource-heavy AI/Video pipeline on free-tier infrastructure using advanced optimization and Docker.",
      "type": "AI Engineering",
      "priority": 4
    },
    {
      "name": "DevPulse: Real-Time Job Tracker",
      "subtitle": "Full-Stack Data Engineering",
      "tech_stack": [
        "Flask",
        "SQLAlchemy",
        "Pandas",
        "Regex NLP",
        "BeautifulSoup",
        "Render"
      ],
      "summary": "A comprehensive data application. Built an automated ETL pipeline that scrapes job data via API, extracts 20+ technical skills using NLP/Regex, creates a relational database, and visualizes live market trends.",
      "live_url": "https://devpulse-job-tracker.onrender.com",
      "github_url": "https://github.com/ntokozo078/devpulse-job-tracker",
      "outcome": "Deployed a self-healing pipeline that tracks demand for skills like Python and AWS in real-time.",
      "type": "Data Engineering",
      "priority": 5
    },
    {
      "name": "Automated Data Extraction Engine",
      "subtitle": "Python ETL Scripting",
      "tech_stack": [
        "Python",
        "Requests",
        "BeautifulSoup",
        "JSON",
        "Pandas"
      ],
      "summary": "A robust scraping engine designed to ingest raw market data from the Adzuna API. It handles pagination, cleans messy HTML content, normalizes currency formats, and structures data for downstream database loading.",
      "github_url": "https://github.com/ntokozo078/devpulse-job-tracker/tree/main/etl",
      "outcome": "Automated the data collection process, reducing manual research time to zero.",
      "type": "Data Engineering",
      "priority": 6
    },
    {
      "name": "Intelligent File System Automator",
      "subtitle": "System Automation",
      "tech_stack": [
        "Python",
        "OS Module",
        "Shutil",
        "Automation"
      ],
      "summary": "A utility script that solves data disorganization. It scans directories, identifies file signatures (extensions), and automatically sorts thousands of files into logical folders (Images, Docs, Code) instantly.",
      "github_url": "https://github.com/ntokozo078",
      "outcome": "Showcased ability to write efficient Python scripts for system administration tasks.",
      "type": "Automation",
      "priority": 7
    },
    {
      "name": "Peer Tutoring Platform",
      "subtitle": "Android + Cloud DB",
      "tech_stack": [
        "Java",
        "Firebase Auth",
        "Firestore NoSQL",
        "XML"
      ],
      "summary": "Developed the backend logic for a tutoring match system. Implemented real-time data syncing using Firestore listeners and secure user authentication.",
      "type": "Mobile Backend",
      "priority": 8
    },
    {
      "name": "Digital Voting System",
      "subtitle": "Secure Backend",
      "tech_stack": [
        "Flask",
        "PostgreSQL",
        "BCrypt",
        "SQLAlchemy"
      ],
      "summary": "Backend voting system with strict integrity constraints. Designed normalized relational models for candidates and votes, ensuring secure and auditable election results.",
      "type": "Full-Stack Engineering",
      "priority": 9
    }
  ],
  "education": {
    "degree": "Bachelor of Information & Communication Technology (BICT)",
    "institution": "Durban University of Technology",
    "year": "Graduated",
    "completion_date": "2025",
    "status": "Completed",
    "key_modules": [
      "Business Intelligence & Data Warehousing",
      "Software Engineering III",
      "Machine Intelligence & AI",
      "Advanced Programming",
      "Cloud Computing Fundamentals",
      "Networks & Cybersecurity Basics",
      "Database Systems"
    ]
  },
  "goals": {
    "career_targets": [
      "Junior Data Engineer",
      "Junior Cloud Engineer",
      "Backend Developer (Data Focus)"
    ],
    "short_term": [
      "Secure a graduate role in 2025",
      "Pass Azure Data Engineer Associate (DP-203)",
      "Build a streaming pipeline with Kafka/Event Hubs"
    ],
    "long_term": [
      "Become a Senior Cloud Data Architect",
      "Specialize in Real-Time Big Data Processing",
      "Lead data infrastructure projects for global tech firms"
    ]
  },
  "strengths": [
    "Bridging Software Engineering & Data Engineering",
    "Building real, deployed applications",
    "Strong Cybersecurity & Network fundamentals",
    "Comfortable with Cloud Workflows (Azure/Render)",
    "Fast learner with a research-driven mindset",
    "Independent problem solver"
  ],
  "areas_for_growth": [
    "Deepen Azure Data Factory practical experience",
    "Implement a real-time streaming project (Kafka)",
    "Advanced SQL performance tuning"
  ]
}